Chapter 1: Bayesian Model
========================================================

# Chapter 1 subindex

* What is a statistical model
* The three problems in Statistics
* Critique of frequentist inference
* Likelihood based Infernce
* Bayesian Model
* Posterior distribution
* Prior predictive and posterior predictive distributions
* Choice of the prior distribution
* Bayesian model as a probability model and as a _data simulator_
* Advantages and disadvantages of going Bayesian

--- Start Class on 11-2-2014 (session 1)

# What is a statistical model

A statistical model (experiment) is a list (a set) of probability models indexed by a parameter that is known to belong to a parameter space $\Omega$.

$M = {P(y | \theta), \theta \in \Omega}$, $\Omega$ parameter space.

when we do inference we assume that data $Y = y$ is coming from a probability model $P(y| \theta^{*})$ that is known to belong to M. We assume that $\theta^{*} \in \Omega$.

We will _claim_ that the model M is correct if the probability model that generated the data $P(y | \theta^{*}) \in M$.


```{r fig.width = 4, fig.height = 4, echo = FALSE}
plot.new()
text(0.5, 0.5, "Missing plot", cex = 1.5)
```


Example 1

$M_{1} = {binomial(n, \theta), \theta \in [0,1]}$

One could also think about using $\Omega = [.1, .9]$ or $\Omega = {.7, .8, .9}$ (trichotomy).

Tossing "pins"

$\theta = P(\bot)$

$1 - \theta = P(\vdash)$

Once you have observed $n = 10$ and you get $y = 4$.

${binomial(10, \theta), \theta \in [0,1]}$ and $y = 4$

* a) Point estimation
    * What is the $\hat \theta$ that best represent your data
* b) Interval estimation
* c) Testing
* d) Prediction of future values

You treat all models as they were equally model _algo mal apuntat_. $\theta = 0$, $\theta = 0.5$, $\theta = 1$. Are all $\theta$ equally credible? 

Implicitly in a statistical model is the assumption that all probability models in $M = {P(y | \theta), \theta \in \Omega}$ are "equally credible". Is that a sensible assumption?

* $\theta = Probability(heads)$
* $\theta = Probability(\bot)$
* $\theta = Probability(blue eyes in Bcn 2014)$

Make sense to do inference in the same way?

```{r fig.width = 4, fig.height = 4, echo = FALSE}
plot.new()
text(0.5, 0.5, "Missing plot \n(probability vs statistics)", cex = 1.5)
```

# The three problems in Statistics

### A) Design of expermients (sampling)

You do not have data. 

Choose the statistical model $M = {P(y | \theta), \theta \in \Omega}$ you are going to obtain your data from out of a list of possible statistical models $M_{1}, M_{2}, \dots M$.

### B) Model Checking (identification)

You have data, $Y = y$, and you have a candidate model $M = {P(y | \theta), \theta \in \Omega}$ and you need to decide whether that model is correct or not. That is, you have to decide whether the probability model $P(y | \theta^{*})$ that generated your data $Y = y$ is in M or not?

### C) Statistical Inference

Given your data $Y = y$ and assuming that the mode $M = {P(y | \theta), \theta \in \Omega}$ is __correct__ (because you know that the model $P(y | \theta^{*})$ generating your data is in M), then you do:

* Point estimation
* Interval estimation
* Testing
* Prediction
* $\dots$

Guessing what $\theta^{*}$ generated your data.

--- Start Class on 13-2-2014 (session 2)

# Critique of frequentist inference

Frequentist statistician is someone doing statistics using only statistical model. Doing that is extremely difficult. 


```{r fig.width = 4, fig.height = 4, echo = FALSE}
plot.new()
text(0.5, 0.5, "Missing plot", cex = 1.5)
```


Goal is to guess (formula) $\theta^{*} \in \Omega$ that generated $Y = y$

* point estimation
* testing
* prediction

### a) Point estimation:

Picking up a function of the data $\hat\theta(y)$ such that is close to the (formula) that generates the data with a large probability.

How do you chose $\hat\theta(y)$?

* $\hat\theta_{ML}(y)$ Maximum likelihood
* $\hat\theta_{MM}(y)$ Moments
* (formula) Least squares. Minimizes a distance between $y, \hat y$
* (formula)

Good asymptotic properties.
All are heuristics.

The only way to rank $\hat\theta(y)$'s and choose one based on how do they perform based on repeated sampling from $M = {p(y| \theta, \theta \in \Omega)}$

Distribution of $\hat\theta(y)$ when $y P()$

```{r dibuix sobre diferents estimadors, fig.width = 4, fig.height = 4, echo = FALSE}
plot.new()
text(0.5, 0.5, "Missing plot \n(diferent estimators)", cex = 1.5)
```


How to choose the best? Based on what?

In order to make this comparison more feasible one often resorts to selecting $\hat\theta(y)$ based on the squared mean error. 

$MSE_{\hat \theta}(\theta) = E((\hat\theta(y) - \theta)^{2}| \theta) = E^{2} + V (formula)$

Problems?
* Difficult to compute.
* Is a function of $\theta$ and you don't know the truth.
* This does not rank your estimator either.

(dibuix sobre mean squared error)

### b) Interval estimation:

What is the subset of $\Omega$ that best represents truth data.

Answer: An interval with confidence p

Difficulties: How do yo build such $C(p) = [] \subset \Omega$

How do you get $formula$.

Its difficult to do it for other thing different than a normal.
The real problem when you have to explain what does it mean that $formula$ has confidence p?

It means that 

$P_{y|\Omega^{*}}()$ (dibuix)

If you repeat the experiment with the same $\theta^{*}$ many times and compute these kind of intervals each time, they would include $\theta^{*}$ with a probability larger than p.

Is a function of $\theta^{*}$, the real value.

Intervals has to be very large in the worst case scenario.

Do we really care only about this?

In practice many people do as if confidence p is the same as 

$P_{\Omega^{*}|y}(d_{L}(y) < ) = P()$ in your actual experiment.

Fixed- random -Fixed This is not confidence intervals.

Confidence Intervals is that is very difficult to compute and understand and interpret. Very hard to justify, like assuming normality and need large sampling.

Paper on binomial distribution.

### c) Testing:

To reduce your parameter space by splitting it in two pieces and pick up one of them 

$\Omega = \Omega_{0} \cup \Omega_{0}$

$H_{0}: \theta \in \Omega_{0}$
$H_{a}: \theta \in \Omega_{a}$

How do you choose a test?

* Likelihood ratio test
* Wald
* Permutation

Only neyman Pearson the only one not asymptotic. But just simple hypothesis. Anything else is just heuristics.

Difficult to choose. Difficult to implement. What do you get with a test? A p-value.

You end up with a [p-value][]. It is very difficult to understand what a p-value means.


You cheat and do as if a p-value = $P(H_{0} is true | data)$

If people want $P(H_{0} is true | data)$ why give a p-value.



[p-value]: http://en.wikipedia.org/wiki/P-value

### The problem with frequentist inference is that:

* It is purely heuristic
* It is grounded on asymptotic results even though N is always finite.
* It is extremely difficult to 
  * justify
  * compute
  * Interpret

Statistics is to difficult to handle starting only from the statistical model.


# Likelihood based Infernce

Between Bayesian Model and Frequentist model.

* Likelihood function
* Can one use a likelihood function as a probability function
* Likelihood principle

### Likelihood function

```{r fig.width = 4, fig.height = 4, echo = FALSE}
plot.new()
text(0.5, 0.5, "Missing plot \n(statistical model)", cex = 1.5)
```

Likelihood function ins a function on $\Omega$ obtained by plugging in your observed data into $P(Y|\theta)$.

$l_{Y = y} (\theta) = P(Y = y | \theta)$

The heuristic behind this is that the largest the $l_{Y = y} (\theta_{i})$, the more likely it is that the data observed comes from $P(y| \Omega^{*} = \Omega_{i})$

```{r fig.width = 4, fig.height = 4, echo = FALSE}
plot.new()
text(0.5, 0.5, "Missing plot", cex = 1.5)
```

The initial idea out of the blue is weird.

#### Example

$\theta = P(\bot) = 1 - P(\vdash)$

$M1 = {binomial(n = (10, \theta), \theta \in [0, \theta])}$ and $y = 6$ 

$\theta_{ML} = 6/10$

$likelihood$

```{r fig.width = 4, fig.height = 4, echo = FALSE}
plot.new()
text(0.5, 0.5, "Missing plot", cex = 1.5)
```

If $l / l = 2$

$\theta_2$ is true more likely to have generated your data from $\theta$

```{r fig.width = 4, fig.height = 4, echo = FALSE}
plot.new()
text(0.5, 0.5, "Missing plot", cex = 1.5)
```

How do you compare subset A with subset B in terms of likelihood?

Can we use areas? Can we read the likelihood function as if it was a probability density function?

There are 2 problems for treating $l(\theta)$ as a pdf.

(integral diferent a 1)

It can be fixed by to a standardized likelihood function.

(integral standardized)

Sometimes is infinite and you can't do that.

If you reparametrize the model 
Probability function have to integrate to 1.

--- Class finishes

### Can one use a likelihood function as a probability function

### Likelihood principle

# Bayesian Model

# Posterior distribution

# Prior predictive and posterior predictive distributions

# Choice of the prior distribution

# Bayesian model as a probability model and as a _data simulator_

# Advantages and disadvantages of going Bayesian
